## 特征值缩放



归一化使得不同 feature 规模一致，使得 $w_i$ 的 GD 变化过程比较一致。

feature scaling，x / MAX

mean normalization，均值归一化，(x - avg)  / (MAX-MIN) 等。。

x-min / max-min

Z-score normalization 标准化，(x - avg) / sigma_i（标准差）标准化后的数据**保持异常值中的有用信息**，使得算法对异常值不太敏感。



靠近 [-1, 1] 大部分也就行。



（1）数据的分布本身就服从**正态分布**，使用Z-Score。

（2）有**离群值**的情况：使用Z-Score。

这里不是说有离群值时使用Z-Score不受影响，而是，Min-Max对于离群值十分敏感，因为离群值的出现，会影响数据中max或min值，从而使Min-Max的效果很差。相比之下，虽然使用Z-Score计算方差和均值的时候仍然会受到离群值的影响，但是相比于Min-Max法，影响会小一点。

（3）如果对输出结果**范围有要求**，用归一化。

（4）如果数据较为稳定，不存在极端的最大最小值，用归一化。

（5）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。





## 梯度下降是否收敛

如果不是连续下降，可能出 bug 了，或者 lr 过大。



## 对特征进行组合 - feature engineering

如，把长和宽组合成面积，保留原本的长和宽，添加面积这一特征，可能会有更好的效果。

同时要注意 feature scaling



np.c_[]  转类型 矩阵转置